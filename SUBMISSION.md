# OmniSense Core

A proactive, privacy-first multimodal "Cognitive Second Brain" for meetings and safety. Built with Next.js + TypeScript and Gemini 3.

## Runtime Instructions
- Requirements: Node 18+, GEMINI_API_KEY set in `.env.local` at project root.
- Development: `npm run dev` ‚Üí http://localhost:3000
- Production Build: `npm run build`
- Production Run: `npm run start` ‚Üí http://localhost:3000

## Key Screens & Flows
- Home: live mic/cam assist, speaking intensity, interruption nudge, suggestions.
- Trainer: edit system instruction, preferences JSON, and history snippet.
- Upload: analyze a video by extracting frames + optional transcript to produce a structured JSON insight with confidence.

## Endpoints
- POST `/api/omnisense/analyze` ‚Äî low-latency JSON insight from live context.
- POST `/api/omnisense/analyze-frames` ‚Äî batch frames + transcript ‚Üí JSON insight with confidence.
- GET  `/api/omnisense/analyze/stream` ‚Äî SSE insight stream (demo behavior).
- GET/POST `/api/omnisense/context` ‚Äî read/write system instruction, preferences, history.
- POST `/api/extract-actions` ‚Äî extract action items from notes.
- POST `/api/suggest` ‚Äî lightweight coaching suggestions.
- GET  `/api/evaluate` ‚Äî synthetic cases for prompt QA, returns per-case outputs + avg confidence.
- GET  `/api/local-video` ‚Äî dev-only stream of local video (C:/Users/USER/Downloads/a.mp4).

## Evaluation & Prompt QA
- Visit `/api/evaluate` to run built-in synthetic scenarios. Use results to refine Trainer context and re-run.

## Security & Hardening
- Basic per-IP rate limiting on main analysis endpoints.
- Schema coercion and a self-check confidence score using Gemini.
- No raw media persisted; context stored in `.data/omni.json`.

## Gemini Usage (200 words)
OmniSense Core uses Gemini 3 as a reasoning engine for multimodal social and safety assistance. The system instruction encodes role, constraints, and safety policy: be concise, avoid sensitive attribute inference, and provide actionable guidance. For live mode, we form lightweight observations (audio intensity, speaking state, interruptions, brief transcript) and call a fast JSON-only prompt to produce a single insight object with fields `insight_type`, `observation`, `analysis`, and `action_recommendation`. For video uploads, we extract a small set of frames (configurable) and include an optional transcript snippet with the same instruction, yielding a consistent JSON schema. To improve reliability, we add a second Gemini call for self-scoring (0‚Äì1) of clarity, actionability, and safety; this produces a confidence score shown to the user. We provide a `/api/evaluate` endpoint with synthetic meeting-like scenarios to stress-test prompts and collect average confidence without external datasets. The Trainer panel enables iterative prompt engineering: edit system instruction, preferences, and history snippet, then re-run evaluation and upload analysis to observe changes. Together, this architecture delivers a robust, privacy-first assistant that demonstrates Gemini 3‚Äôs multimodal capabilities in a focused, judge-friendly MVP.

## Demo Script (Suggested)
1) Open Upload and analyze sample video; show insight + confidence.
2) Paste notes ‚Üí extract actions.
3) Open Trainer, tweak instruction, re-run `/api/evaluate`.
4) Switch to Home; show speaking intensity and suggestions.

## Judge Demo Checklist (Production)

Live URL: https://omnisense-orchestrator.vercel.app

1. Header setup
   - Set Privacy: Cloud.
   - (Optional) Enable Conversational Voice for micro‚Äëcoaching.
2. Start Judge Demo
   - Click ‚ÄúStart Judge Demo‚Äù (header) ‚Üí runs a 3‚Äëstep autonomous loop and exports an HTML report.
   - Wait for inline status: ‚ÄúDemo complete. Report: ‚Ä¶‚Äù.
3. Verification / Audit
   - Open the Verification/Audit panel.
   - Click ‚ÄúRefresh Timeline‚Äù to load session events and verify steps.
   - Click ‚ÄúExport HTML Report‚Äù; note the artifact path returned.
   - Use ‚ÄúRefresh‚Äù in Artifacts to list recent report/run files.
4. Temporal coaching (optional)
   - Speak continuously ~10s to trigger Dominance; listen for a brief coaching cue.
   - Create a short spike to simulate Overlap.

## Production Smoke Test

1. Health
   - GET /api/health ‚Üí { "status": "ok" }
2. Agent Single Step
   - POST /api/agent/act with a minimal observation, e.g. { observation: { detection: { kind: "overlap" } }, maxTools: 1 }
   - Expect JSON with thoughts, toolCalls[], and signature/level.
3. Agent Run
   - POST /api/agent/run with { goal: "Prepare follow-up plan", steps: 2 }
   - Expect { ok: true, steps, artifact } and verify an artifact path.
4. Audit Endpoints
   - GET /api/audit/timeline ‚Üí session, tasks, logs, verifySteps.
   - POST /api/audit/report ‚Üí { ok: true, artifact } (HTML report path).
5. Research Provider (optional)
   - If GOOGLE_API_KEY + GOOGLE_CSE_ID set: web.search tool results include Google CSE summaries.
   - Otherwise falls back to Wikipedia summaries.

---

## Hackathon Submission Text (copy/paste)

**What we built (short description)**  
OmniSense is a **live social translator** that listens through mic, camera, and AI glasses, explains the room‚Äôs social subtext in four lines, and then uses a **Gemini 3‚Äëpowered agent** to handle all the follow‚Äëups. It is not a chat wrapper: it is a tool‚Äëcalling orchestrator with long‚Äëterm memory, policies, and verification.

**Strategic track fit**  
- **üß† Marathon Agent** ‚Äì Planner + Action Queue + long‚Äëterm memory form a background agent that keeps drafting goals, creating tasks, and maintaining continuity across days of interactions.  
- **üë®‚Äçüè´ Real‚ÄëTime Teacher** ‚Äì Live mic/cam + glasses sensors drive streaming ‚Äúsocial translation‚Äù for classrooms and meetings: The Vibe, Hidden Meaning, Social Red Flags, and The Social Script.  
- **‚òØÔ∏è Vibe Engineering** ‚Äì Thought signatures, verification tools (`agent.verify_step`), synthetic eval (`/api/evaluate`), and HTML audit artifacts build a self‚Äëevaluation loop that measures and improves behavior over time.

**How we use Gemini 3 (‚âà200 words)**  
OmniSense uses Gemini 3 Pro as a **tool-calling orchestrator**, not a simple chat wrapper. For live ‚Äúsocial translation‚Äù, the browser streams lightweight observations (RMS intensity, speaking state, interruptions, glasses sensor cues, short transcript snippets) to `/api/omnisense/analyze/stream`. The server builds a rich prompt that combines system instruction, user preferences, long-context assembly, and a long‚Äëterm memory snippet. Gemini 3 Pro returns a structured JSON object with a four-line script: The Vibe, The Hidden Meaning, Social Red Flags, The Social Script. A second Gemini call self-scores clarity, actionability, and safety, feeding a confidence bar and conservative/proactive tuning.  
For the **Marathon Agent**, `/api/agent/run` calls `runAgentStep`, which provides Gemini 3 with tool schemas (`web.search`, `calendar.create_event`, `memory.write`, `tasks.create`, `tasks.update_status`, `notes.write`, `agent.verify_step`) plus long-term memory and preferences. Gemini proposes tool_calls; the server executes them, logs thought signatures, and writes verification records. `/api/evaluate` uses Gemini to synthesize diverse scenarios and rubric-score OmniSense‚Äôs outputs across four competencies, producing quantitative reliability metrics. Together, this architecture showcases Gemini 3 Pro‚Äôs long context, multimodal reasoning, structured outputs, and agentic tool calling in a way that is self-evaluating and ready for real-world social coaching.

